{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework10\n",
    "\n",
    "Exercises with text processing and NLP modeling\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Understand similarities and differences between the processes of working with text, images and tabular data\n",
    "- Practice with different methods of encoding and modeling text data\n",
    "- See different methods for extracting information or patterns from text datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/text_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from data_utils import display_silhouette_plots, object_from_json_url\n",
    "from text_utils import get_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell it's gonna be a good homework from the number of imports.\n",
    "# ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have protein, need seasoning\n",
    "\n",
    "Let's create a model to help us season our foods. In the end, what we want is a model that receives a short list of ingredients and returns a list of seasonings or complementary ingredients for our original ingredients list.\n",
    "\n",
    "In order to do that we need a dataset of recipes. We'll load that into a text dataset where each recipe is a document and the ingredients are our document *tokens*.\n",
    "\n",
    "Let's take a look at the recipe dataset and become familiar with the data and how it's organized.\n",
    "\n",
    "We'll load our recipes and do a bit of exploratory data analysis to look for patterns first to see if this kind of modeling makes any sense.\n",
    "\n",
    "### Load Data\n",
    "\n",
    "Here's our dataset. Let's load it into an object for inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/refs/heads/main/datasets/text/recipes\"\n",
    "recipes_obj = object_from_json_url(f\"{DATAPATH}/recipes_min16.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Data\n",
    "\n",
    "How's the data organized?\n",
    "\n",
    "How many recipes do we have?\n",
    "\n",
    "Do all recipes have the same number of ingredients?\n",
    "\n",
    "Anything else stand out about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Look at Data here\n",
    "print(f\"Shape of recipes_obj: {len(recipes_obj)}\")\n",
    "print(f\"First recipe: {recipes_obj[0]}\")\n",
    "\n",
    "\n",
    "\n",
    "# TODO: How many recipes\n",
    "\n",
    "# TODO: How many ingredients do the shortest and longest recipes have?\n",
    "# Find recipes with min and max number of ingredients\n",
    "min_recipe = min(recipes_obj, key=lambda x: len(x['ingredients']))\n",
    "max_recipe = max(recipes_obj, key=lambda x: len(x['ingredients']))\n",
    "\n",
    "print(\"\\nRecipe with fewest ingredients:\")\n",
    "print(min_recipe)\n",
    "print(f\"\\nNumber of ingredients: {len(min_recipe['ingredients'])}\")\n",
    "\n",
    "print(\"\\nRecipe with most ingredients:\")\n",
    "print(max_recipe) \n",
    "print(f\"\\nNumber of ingredients: {len(max_recipe['ingredients'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Features\n",
    "\n",
    "Our dataset doesn't really have to be a `DataFrame` here. It can, but it doesn't have to be.\n",
    "\n",
    "Each recipe right now is described as a list of ingredients, but what we really want is a list of *sentences*, where each *sentence* is a Python `string` with all of the ingredients for a given recipe.\n",
    "\n",
    "Instead of:<br>```[\"salt\", \"baking soda\", \"water\", \"mushroom\"]```,\n",
    "\n",
    "we want:<br>```\"salt baking soda water mushroom\"```\n",
    "\n",
    "The `join()` function might help.\n",
    "\n",
    "Another thing to consider is wether we want to do anything special about multi-word ingredients, like *baking soda*.\n",
    "\n",
    "Do we want to let our vectorizer (spoiler) split that into two tokens, or do we want to guarantee that *baking* and *soda* always stay together? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: turn list of objects into list of strings\n",
    "# Convert list of ingredients to space-separated strings\n",
    "recipe_strings = [' '.join(recipe['ingredients']) for recipe in recipes_obj]\n",
    "\n",
    "print(recipe_strings[0])\n",
    "\n",
    "# Create new recipes_obj with recipe strings instead of ingredient lists\n",
    "recipes_obj_strings = []\n",
    "for i, recipe in enumerate(recipes_obj):\n",
    "    # Create a copy of the recipe dict and replace ingredients with the string version\n",
    "    new_recipe = recipe.copy()\n",
    "    new_recipe['ingredients'] = recipe_strings[i]\n",
    "    recipes_obj_strings.append(new_recipe)\n",
    "\n",
    "print(\"\\nFirst recipe with string ingredients:\")\n",
    "print(recipes_obj_strings[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Data\n",
    "\n",
    "The fun part.\n",
    "\n",
    "Let's vectorize our list of ingredient strings into a sparse document matrix using `CountVectorizer` or `TfidfVectorizer`.\n",
    "\n",
    "The resulting matrix will have one row for each recipe, and the columns will encode the ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Vectorize ingredients from our recipe list\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the recipe strings\n",
    "recipe_matrix = vectorizer.fit_transform(recipe_strings)\n",
    "\n",
    "# Print the shape of the resulting matrix\n",
    "print(f\"Shape of recipe matrix: {recipe_matrix.shape}\")\n",
    "\n",
    "# Print the vocabulary (ingredients)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "print(\"\\nVocabulary (ingredients):\")\n",
    "print(vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Data\n",
    "\n",
    "Now that we have our recipes/documents vectorized we can study them a little bit, and look for patterns.\n",
    "\n",
    "What happens if we cluster our recipes ? What do the cluster centers represent ?\n",
    "\n",
    "When might this be useful ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: cluster recipes\n",
    "# Create a KMeans object\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "\n",
    "# Fit the KMeans model\n",
    "kmeans.fit(recipe_matrix)\n",
    "\n",
    "# Get the cluster assignments for each recipe\n",
    "recipe_clusters = kmeans.labels_\n",
    "\n",
    "# Print the number of recipes in each cluster\n",
    "for i in range(10):\n",
    "    cluster_size = sum(recipe_clusters == i)\n",
    "    print(f\"Cluster {i}: {cluster_size} recipes\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Centers\n",
    "\n",
    "Use the `get_top_words()` function to decode the `cluster_centers` back into ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Check shapes\n",
    "# print(\"Vocabulary size:\", len(vocabulary))\n",
    "# print(\"Cluster centers shape:\", kmeans.cluster_centers_.shape)\n",
    "\n",
    "# # kmeans.cluster_centers_\n",
    "\n",
    "# # For each cluster center\n",
    "# for i, center in enumerate(kmeans.cluster_centers_):\n",
    "#     print(f\"\\nCluster {i}:\")\n",
    "#     # Get the indices of the top 8 values in this center\n",
    "#     top_indices = center.argsort()[-8:][::-1]\n",
    "#     # Print the corresponding words and their values\n",
    "#     for idx in top_indices:\n",
    "#         print(f\"{vocabulary[idx]}: {center[idx]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "from text_utils import get_top_words\n",
    "\n",
    "# TODO: Look at cluster centers\n",
    "get_top_words(kmeans.cluster_centers_, vocabulary, 8)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "What do these cluster centers represent ?<br>\n",
    "Is there anything interesting about recipe cluster centers ?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "It looks like it's STARTING to center around different cuisines. There are a couple that could be classified as mexican food, some east asian in nature, another more Italian, and of course baking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Clusters\n",
    "\n",
    "Let's plot our clusters to see if we have to adjust any of the clustering parameters.\n",
    "\n",
    "Since we can't plot in $500$ dimensions, we should use `PCA` to look at our clusters in $2D$ and $3D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: PCA to reduce the dimensions of our feature space\n",
    "\n",
    "pca_2d = PCA(n_components=2).fit_transform(recipe_matrix)\n",
    "pca_3d = PCA(n_components=3).fit_transform(recipe_matrix)\n",
    "\n",
    "# Plotting 2D PCA\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=recipe_clusters, cmap='viridis')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('2D PCA of Recipe Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D PCA\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(pca_3d[:, 0], pca_3d[:, 1], pca_3d[:, 2], c=recipe_clusters, cmap='viridis')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.set_title('3D PCA of Recipe Clusters')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: plot clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Silhouette Plots\n",
    "\n",
    "We can also check the quality of our clustering by looking at the silhouette plots that we get from calling:<br>\n",
    "`display_silhouette_plots(vectors, clusters)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_silhouette_plots(recipe_matrix, recipe_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "How many clusters did you end up with ?<br>\n",
    "How do they look ?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "10 clusters... they look ok! The silhouette plot indicates that most clusters have most of their points effectively clustered... but the best are clusters 2, 5, 6, and 9. The sizing of the clusters isnt even but there arent any seriously significant outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe Completion\n",
    "\n",
    "Ok. On to the main event.\n",
    "\n",
    "Let's create some recipes.\n",
    "\n",
    "We'll do this using a technique similar to what is used for movie/product recommendations. Given an initial set of ingredients, we'll look at recipes that have similar ingredients and \"recommend\" additional ingredients.\n",
    "\n",
    "We already have all of the recipes in our dataset encoded as `tf-idf` vectors. The rest of our algorithm will be something like:\n",
    "1. Start with an initial set of ingredients\n",
    "2. Encode ingredients\n",
    "3. Find a set of recipes that are similar to our list of ingredients\n",
    "4. Find common ingredients that are in the similar recipes, but not in our list of ingredients\n",
    "5. Pick representative ingredient to add to recipe\n",
    "6. Repeat\n",
    "\n",
    "Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial list of ingredients\n",
    "\n",
    "This is just a string with ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_seed_str = \"sesame onion salt\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encode ingredients\n",
    "\n",
    "Transform the string into a `tf-idf` vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.5773502691896258\n",
      "  (0, 0)\t0.5773502691896258\n",
      "  (0, 1)\t0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "# TODO: transform string into sparse vector\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "recipe_seed_vct = vectorizer.fit_transform([recipe_seed_str])\n",
    "\n",
    "print(recipe_seed_vct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe matrix:   (0, 331)\t0.556086189023217\n",
      "  (0, 25)\t0.43552190735789215\n",
      "  (0, 319)\t0.38363852360479905\n",
      "  (0, 144)\t0.28904072157171146\n",
      "  (0, 402)\t0.262441547761677\n",
      "  (0, 249)\t0.3365467253208089\n",
      "  (0, 162)\t0.2970294577191098\n",
      "  (1, 286)\t0.2993109651960138\n",
      "  (1, 79)\t0.2164309158304764\n",
      "  (1, 353)\t0.1346738081660667\n",
      "  (1, 115)\t0.47768401910946195\n",
      "  (1, 41)\t0.21529596894597175\n",
      "  (1, 301)\t0.15331363862860353\n",
      "  (1, 356)\t0.430411570723536\n",
      "  (1, 272)\t0.20316483500696192\n",
      "  (1, 270)\t0.1443124395406509\n",
      "  (1, 221)\t0.4984282556496909\n",
      "  (1, 445)\t0.2271722813229876\n",
      "  (2, 25)\t0.3025612256912186\n",
      "  (2, 319)\t0.26651734381952236\n",
      "  (2, 402)\t0.18232064798940725\n",
      "  (2, 162)\t0.20634919914612035\n",
      "  (2, 353)\t0.11535127261900756\n",
      "  (2, 113)\t0.24887736777901448\n",
      "  (2, 395)\t0.308206539747551\n",
      "  :\t:\n",
      "  (5012, 73)\t0.4043922982783898\n",
      "  (5013, 353)\t0.15010578165038468\n",
      "  (5013, 58)\t0.25985640311055863\n",
      "  (5013, 354)\t0.48151008376719673\n",
      "  (5013, 318)\t0.3746988761824002\n",
      "  (5013, 199)\t0.4383463800179014\n",
      "  (5013, 0)\t0.5878300320848919\n",
      "  (5014, 249)\t0.19445660189460207\n",
      "  (5014, 79)\t0.15418161994632676\n",
      "  (5014, 353)\t0.09593927848851772\n",
      "  (5014, 41)\t0.15337310352650516\n",
      "  (5014, 445)\t0.1618335818932426\n",
      "  (5014, 58)\t0.16608577998091062\n",
      "  (5014, 82)\t0.1828302647592154\n",
      "  (5014, 54)\t0.20172919291495614\n",
      "  (5014, 77)\t0.251192969526211\n",
      "  (5014, 285)\t0.2525568449912868\n",
      "  (5014, 358)\t0.263810809665529\n",
      "  (5014, 74)\t0.24559169372998235\n",
      "  (5014, 425)\t0.15892625473819275\n",
      "  (5014, 180)\t0.26408433767039136\n",
      "  (5014, 335)\t0.25209904428035473\n",
      "  (5014, 31)\t0.26981049670273743\n",
      "  (5014, 182)\t0.3633105595078127\n",
      "  (5014, 408)\t0.3970544780115649\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the recipe strings\n",
    "recipe_matrix = vectorizer.fit_transform(recipe_strings)\n",
    "\n",
    "# Print the shape of the resulting matrix\n",
    "print(f\"recipe matrix: {recipe_matrix}\")\n",
    "\n",
    "# # Print the vocabulary (ingredients)\n",
    "# vocabulary = vectorizer.get_feature_names_out()\n",
    "# print(\"\\nVocabulary (ingredients):\")\n",
    "# print(vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find similar recipes\n",
    "\n",
    "The meat of the algorithm. No pun intended.\n",
    "\n",
    "In order to find similar recipes, we'll first calculate the distance between our current list of ingredients and all recipes in our dataset.\n",
    "\n",
    "We can start with euclidean distance and later try other kinds, but the overall processing will be the same:\n",
    "\n",
    "1. Start with an empty list to store distances\n",
    "2. Loop over the `tf-idf` recipe vectors and for each vector:\n",
    "   1. Subtract the ingredient list\n",
    "   2. Square the difference (to square a sparse matrix `A`, use `A.multiply(A)`)\n",
    "   3. Sum the terms of the result\n",
    "   4. Take the square root of the sum\n",
    "   5. Append to distance list\n",
    "3. Find the indices of the smallest distances (this operation is called `argsort` and will give us the indices of the recipes that are most similar to our list of ingredients)\n",
    "4. Check the recipes to see if they are indeed similar (`inverse_transform()` the vectors at the indices calculated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argsort a list (get sequence of indices that would sort the list)\n",
    "# https://stackoverflow.com/a/3382369\n",
    "def argsort(L, reverse=False):\n",
    "  return sorted(range(len(L)), key=L.__getitem__, reverse=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salt spinach sesame oil garlic sesame seeds soy sauce onion', 'salt red pepper sesame oil vinegar sesame seeds cucumber onion sugar', 'sesame oil tuna sesame seeds soy sauce lime juice onion', 'spinach sesame oil garlic onion sesame seeds soy sauce carrots mushroom onion olive oil sugar']\n"
     ]
    }
   ],
   "source": [
    "# create empty list called recipe_dists, then loop over recipe_matrix (defined above as a tfidf vectors) and for each vector subtract recipe_seed_vct, then square the difference, then sum the terms, then take the square root of the sum, and then append the result to recipe_dists\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# First create and fit the vectorizer on all recipes\n",
    "vectorizer = TfidfVectorizer()\n",
    "recipe_matrix = vectorizer.fit_transform(recipe_strings)\n",
    "\n",
    "# Then transform your seed recipe using the SAME vectorizer\n",
    "recipe_seed_vct = vectorizer.transform([recipe_seed_str])\n",
    "\n",
    "# Now calculate distances\n",
    "recipe_dists = []\n",
    "for i in range(recipe_matrix.shape[0]):\n",
    "    recipe_vct = recipe_matrix[i]\n",
    "    diff = recipe_vct - recipe_seed_vct\n",
    "    squared = diff.multiply(diff)\n",
    "    dist = np.sqrt(squared.sum())\n",
    "    recipe_dists.append(dist)\n",
    "\n",
    "# Find most similar recipes\n",
    "most_similar_indices = np.argsort(recipe_dists)[:16]\n",
    "most_similar_recipes = [recipe_strings[i] for i in most_similar_indices]\n",
    "\n",
    "print(most_similar_recipes[:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find ingredients to recommend\n",
    "\n",
    "We have a way to get a set of similar recipes with similar ingredients, and now want to find a *meaningful*, or *representative*, ingredient to add to our ingredients list.\n",
    "\n",
    "Let's consider ingredients in the $16$ most similar recipes. What we are trying to do is find an ingredient that is in a lot of these recipes, but not yet in our list of ingredients.\n",
    "\n",
    "There are many possible ways of doing this. We could count the number of times different ingredients show up in these $16$ recipes using Python dictionaries and/or sets, but what we're trying to do here is very similar to what a `TfidfVectorizer` does: calculate relative importance of terms in a series of documents.\n",
    "\n",
    "Let's re-encode these $16$ recipes using their own separate `TfidfVectorizer`, then sum the importance of each ingredient and look at ingredients with the highest importance scores.\n",
    "\n",
    "We could re-use the vectors/scores from the original `TfidfVectorizer`, but they're gonna be influenced by the relative frequencies of all of the ingredients that showed up in all of the recipes. Using a separate vectorizer is a little bit more precise.\n",
    "\n",
    "The steps we need to take are:\n",
    "\n",
    "1. Separate the $16$ recipes most similar to our list of ingredients\n",
    "   1. We have lots of representations of our recipes, but `recipes` (list of strings) might be the easiest one to use here\n",
    "2. Create a new `TfidfVectorizer` and encode the $16$ recipes\n",
    "3. Sum the resulting vectors to get overall importance scores for each ingredient/token\n",
    "4. Convert resulting vector to a list using `A.tolist()[0]`\n",
    "5. `argsort` the importance scores to get sequence of ingredient indices ordered from most to least important\n",
    "6. Find the most important ingredient that isn't on the ingredient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4481433840146247, 0.4674231586020272, 1.2014419770083555, 0.389161984386447, 0.42343336754728267, 0.6606720364686809, 0.389161984386447, 0.7567516048966487, 0.39707956073919143, 1.367959055330107, 0.7230298990436166, 2.116483651682748, 0.7747571651260162, 0.34006864199899384, 0.4912523601215304, 0.39707956073919143, 0.4901664362438052, 0.4912523601215304, 0.4481433840146247, 0.41856264090638406, 0.8099848298780381, 3.1892214825480303, 0.9041200110445207, 2.5871058575453505, 0.34006864199899384, 1.4993213650764967, 0.39707956073919143, 0.42343336754728267, 1.1296102708429427, 0.389161984386447, 0.4674231586020272, 0.7230298990436166, 1.5836415050508175, 2.413134458915745, 1.226133952809284, 2.581660374847202, 5.055020916413073, 2.413134458915745, 1.0930030882877448, 0.4481433840146247, 2.042695175730975, 1.3081909456491392, 0.4950136019513097, 0.4912523601215304, 0.389161984386447, 1.4280564224825993]\n",
      "[36, 21, 23, 35, 33, 37, 11, 40, 32, 25, 45, 9, 41, 34, 2, 28, 38, 22, 20, 12, 7, 10, 31, 5, 42, 14, 17, 43, 16, 1, 30, 0, 18, 39, 4, 27, 19, 8, 15, 26, 3, 6, 29, 44, 13, 24]\n",
      "Most important ingredient to add: oil\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get 16 most similar recipes\n",
    "most_similar_recipes = most_similar_recipes[:16]\n",
    "# print(most_similar_recipes)\n",
    "\n",
    "# TODO: Encode the 16 recipes\n",
    "# First create and fit the vectorizer on all recipes\n",
    "vectorizer = TfidfVectorizer()\n",
    "most_similar_matrix = vectorizer.fit_transform(most_similar_recipes)\n",
    "# print(most_similar_matrix[0])\n",
    "\n",
    "# TODO: Sum the recipe vectors by column to get ingredient importance scores\n",
    "\n",
    "# Sum columns of the sparse matrix to get ingredient importance scores\n",
    "importance_scores = most_similar_matrix.sum(axis=0)\n",
    "# print(importance_scores)\n",
    "\n",
    "# TODO: Convert sparse vector to regular list with A.tolist()[0]\n",
    "importance_scores_list = importance_scores.tolist()[0]\n",
    "print(importance_scores_list)\n",
    "\n",
    "# TODO: argsort the importance scores\n",
    "sorted_indices = argsort(importance_scores_list, reverse=True)\n",
    "print(sorted_indices)\n",
    "\n",
    "# TODO: Find most important ingredient not yet on the list of ingredients\n",
    "ingredients_list = recipe_seed_str.split()  # Split current ingredients into list\n",
    "feature_names = vectorizer.get_feature_names_out()  # Get ingredient names from vectorizer\n",
    "\n",
    "# Loop through sorted indices to find first ingredient not in current list\n",
    "for idx in sorted_indices:\n",
    "    ingredient = feature_names[idx]\n",
    "    if ingredient not in ingredients_list:\n",
    "        most_important_ingredient = ingredient\n",
    "        break\n",
    "        \n",
    "print(f\"Most important ingredient to add: {most_important_ingredient}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add ingredient to recipe\n",
    "\n",
    "This is simply adding a word to `recipe_seed_str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated recipe ingredients: sesame onion salt oil\n"
     ]
    }
   ],
   "source": [
    "# TODO: add the first important ingredient to list of ingredients\n",
    "recipe_seed_str += \" \" + most_important_ingredient\n",
    "print(f\"Updated recipe ingredients: {recipe_seed_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Repeat (Optional)\n",
    "\n",
    "Now we can repeat this process until we get an empty list of important ingredients: \n",
    "1. Encode current recipe\n",
    "2. Find similar recipes\n",
    "3. Find important ingredients\n",
    "4. Add important ingredient\n",
    "\n",
    "Might be helpful to define a couple of functions, like `find_similar_recipes()` and `find_important_ingredients()`...\n",
    "\n",
    "Only do this step if you're really curious about experimenting with generating unconventional ingredient lists. It's not going to be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Create find_similar_recipes(ingredients, recipes, vectorizer)\n",
    "\n",
    "# TODO: Create find_important_ingredients(recipes)\n",
    "\n",
    "# TODO: Create recipe by repeating calls to find_similar_recipes() and find_important_ingredients()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
